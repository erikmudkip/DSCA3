{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing and Random Forest Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a branch of artificial intellegence (AI). MAchine learning itself is a scientific study of algorithms and statistical models in computer system that i used to perform a specific rask without using any explicit instructions. This is done by using patters and inference from the datas that are provided as the test suite.\n",
    "\n",
    "There are diffrent type of machine learning methods, in which supervised learning and unsupervised learning are the most widely adopted machine learning methods. other most popular machine learning method are semisupervised learning and reinforced learning. In this tutorial we will be focusing on supervised learning method. Supervised learning method is where the model is given a set of training datas where the data have a set of inputs and output such as True or False. Supervised learning can furthermore be grouped into classification and regression. Each of them have different use, for exaple classification is used to classifisy values while regression in used to forecast values.\n",
    "\n",
    "Another important aspect of machine learning is the algorithms used. some of the most popular machine learning algorithms currently are K-nearest neighbours, Random Forest, Neural Network, Decision Tree and Linear Regression. In this tutorial we will dive into Random forest. Random forest itself is an ensemble learning that create multiple unique decision trees with different variables. All of this decision tree then will create a result and the average of those comulative results will be used as the output of random forest algorithm.\n",
    "\n",
    "After this tutorial you will be able to:\n",
    "\n",
    "- Pre-process data\n",
    "- Understand how random forest work\n",
    "- Understand what each attributes do in random forest parameter\n",
    "- Improving the accuracy of random forest output\n",
    "- Solving underfitting and overfitting problem in random forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dota is a popular online multiplayer online battle arena(MOBA) game developed by Valve. its is a game where 2 teams whcih consist of 5 players each pick a character from a pool of 113 playable heroes and race to destroy the enemy ancient. Each of the hero is diffrent and provide diffrent advantage to the team. The tutorial will show how to make a machine learning model that can predict the outcome of the match based on the heroes pick.\n",
    "\n",
    "The dataset that we will be using was taken from UCI (https://archive.ics.uci.edu/ml/datasets/Dota2+Games+Results). The datasets consist of training dataset and test dataset. The training dataset consist of 92650 rows of data while the test dataset consist of 10294 rows of data. \n",
    "\n",
    "The dataset rows represent diffrent aspect of the game:\n",
    "- The first collumn represent the output of the match. the value could be 1 which represent the first team is the winner or -1 that represent the second team is the winner of the game\n",
    "- The second collumn represent the cluster id of where the game is played. the values range from 111 to 261.\n",
    "- The third collumn represent the game mode on how the heroes is picked and how big is the hero pool. the values range from -1 to 8\n",
    "- The fourth collumn represent the game type such as ranked or unranked game. the values range from 1 to 3.\n",
    "- the rest of the 113 collum represent each playable characters in the game where value 1 is for the characters picked by team 1, -1 for characters picked by team 2 while 0 for characters that are not picked in that game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.\n",
    "\n",
    "Data preprocessing have to go trough series of step during pre-processing step:\n",
    "- Data cleaning, this process cleansed the data by filling in missing values, smoothing noisy data and resolving the inconsistency of the data. the most common method to clean data with missing value is by removing the entire row of data\n",
    "- Data integration, this process is done when you have 2 or more different datasets. this process combine both datasets and resol ve the conflict within the data\n",
    "- Data transformation, in this process datas are normalized, aggregated and generalized.\n",
    "- Data reduction, in this process the data size is reduced to fit the requirment and prevent overfitting in the model.\n",
    "- Data discretization, in this process data values are limited to certain range. this is done to minimalize the loss of datas\n",
    "\n",
    "we will beusing the step based on our needs, therefore its not often that only 1 or 2 step used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are importing the important libraries that we will be using for this tutorial\n",
    "\n",
    "- pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series\n",
    "- NumPy is the fundamental package for scientific computing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"dota2train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   107  108  109  110  \\\n",
       "0   -1  223    2    2    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    1  152    2    2    0    0    0    1    0   -1 ...     0    0    0    0   \n",
       "2    1  131    2    2    0    0    0    1    0   -1 ...     0    0    0    0   \n",
       "\n",
       "   111  112  113  114  115  116  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "\n",
       "[3 rows x 117 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_data[train_data.columns[0]]\n",
    "X = train_data.drop(train_data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1\n",
       "1    1\n",
       "2    1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1    2    3    4    5    6    7    8    9    10  ...   107  108  109  110  \\\n",
       "0  223    2    2    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1  152    2    2    0    0    0    1    0   -1    0 ...     0    0    0    0   \n",
       "2  131    2    2    0    0    0    1    0   -1    0 ...     0    0    0    0   \n",
       "\n",
       "   111  112  113  114  115  116  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "\n",
       "[3 rows x 116 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are importing the training dataset and testing dataset that we will be using which is \"dota2train.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "87     0\n",
       "88     0\n",
       "89     0\n",
       "90     0\n",
       "91     0\n",
       "92     0\n",
       "93     0\n",
       "94     0\n",
       "95     0\n",
       "96     0\n",
       "97     0\n",
       "98     0\n",
       "99     0\n",
       "100    0\n",
       "101    0\n",
       "102    0\n",
       "103    0\n",
       "104    0\n",
       "105    0\n",
       "106    0\n",
       "107    0\n",
       "108    0\n",
       "109    0\n",
       "110    0\n",
       "111    0\n",
       "112    0\n",
       "113    0\n",
       "114    0\n",
       "115    0\n",
       "116    0\n",
       "Length: 117, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are checking if out dataset have any missing value, but as you can clearly seen all of our data are complete. there are several way to fix the dataset if we find some missing values such as:\n",
    "- Listwise deletion: remove data that have missing values\n",
    "- Recover the values: you can try to contact the participant of the survey to fill in the missing values\n",
    "- Educated guessing: you can try to make a good guess of what should be in the missing values\n",
    "- Average imputation: we can use the average of the dataset values to fill in the missing values\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting the dataset into training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will be splitting our procured dataset into training and test set using train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we need to import the train_test_split from sklearn to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the 4 variables name that we will be using to diffrentiate the value. Then we call the train_test_split method. the parameter required to be inputted are (\"input dataset\", \"output datatset\", \"the percentage of the test dataset\"). you dont want to put the test size too big since this can cause an underfitting. I suggest to give the test dataset a split the dataset with the ratio of 70:30 or 80:20. the 0.2 represent the 80:20 ratio while if we change the test_size to 0.3, it will represent 70:30 split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43356</th>\n",
       "      <td>183</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89449</th>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10726</th>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3    4    5    6    7    8    9    10  ...   107  108  109  \\\n",
       "43356  183    9    2    0   -1    0    1    0   -1    0 ...     0    0    0   \n",
       "89449  144    2    3    1    0    0    0    0   -1    0 ...     1    0    0   \n",
       "10726  151    2    2    0    1    0    0    0    0    0 ...     0    0    0   \n",
       "\n",
       "       110  111  112  113  114  115  116  \n",
       "43356    0    0    0    0    0   -1    0  \n",
       "89449    0    0    0    0    0    0    0  \n",
       "10726    0    0    0    0    0    0    0  \n",
       "\n",
       "[3 rows x 116 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77565</th>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51525</th>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80811</th>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1    2    3    4    5    6    7    8    9    10  ...   107  108  109  \\\n",
       "77565  231    1    2    0    0    0    0    0    0    0 ...     0    0    0   \n",
       "51525  121    2    2    0    0   -1    0    0    0    0 ...     1    0    0   \n",
       "80811  156    2    2    1    0    0    0    0    0    0 ...     0    0    0   \n",
       "\n",
       "       110  111  112  113  114  115  116  \n",
       "77565    0    0    0    0    0    0    0  \n",
       "51525    0    0    0    0    0    0    0  \n",
       "80811    0    0    0    0    0    0    0  \n",
       "\n",
       "[3 rows x 116 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43356    1\n",
       "89449   -1\n",
       "10726    1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77565    1\n",
       "51525    1\n",
       "80811    1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see clearly above are the result of the train_test_split method that we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How random forest work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain it simply random forest algorithm create multiple decision tree and merge them togather to increase the accuracy of the prediction. for exaple imagine you have a decision tree that looks like the picture below. then imagine you have 99 more of this decision tree but each with a diffrent question such as \"any alcohol?\" instead of \"any girls?\". then all of those 100 decision tree will produce a result which is \"Go\" or \"Won't go\" and the result you receive is 67 for \"Go\" and 33 for \"Won't go\". Like the majority votes rule, the output will be \"Go\" for the random forest output."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple implementation of random forest classifier. we will use the classifier to decide if the first team will win or the second team will win based on the characters pick and etc. Keep in mind we don't use the regression because we cant really predict the outcomes of the game. This is caused due:\n",
    "1. Each player have diffrent skill\n",
    "2. Emotion affect the player performance\n",
    "3. etc\n",
    "\n",
    "Therefore we dont enough data to decide who will win overall, but we can classify who's more likely to win based on their characters pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the random forest classifier from scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20, random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really basic random forest classifier in which we set the numebr of trees to 20 and no random state. There are a lot of parameter that affect random forest classifier. We will talk about what each parameter do in another section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are fitting the training dataset that we split on the pre-processing section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tell our fitted model to predict the outcome of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model Accuracy...\n",
      "Training Accuracy =  0.998974635726\n",
      "Test Accuracy =  0.549001618996\n",
      "\n",
      "\n",
      "Printing predicted result...\n",
      "Result_of_Treatment =  [ 1  1 -1 ...,  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting Model Accuracy...\")\n",
    "\n",
    "# Get regression details\n",
    "#print(\"Estimated Coefficient = \", regressor.coef_)\n",
    "#print(\"Estimated Intercept = \", regressor.intercept_)\n",
    "print(\"Training Accuracy = \", rf.score(x_train, y_train))\n",
    "print(\"Test Accuracy = \", rf.score(x_test, y_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing predicted result...\")\n",
    "print(\"Result_of_Treatment = \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In here we are checking the accuracy of our test set which is 55%. Thats a bit disappointing isn't it? We will fix it in the next section and try to increase the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffrent dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will compare the accuracy of our model that's trained using the Dota2 dataset the Iris dataset. The dataset we will be using is taken from \"https://www.kaggle.com/saurabh00007/iriscsv\". we will be using the same treatment and parameter from dota2 dataset and apply them to the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iris = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_data['species']\n",
    "X = train_data.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameter in random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parameters in random forest classifier:\n",
    "- n_estimators\n",
    "- criterion\n",
    "- max_features\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "- min_weight_fraction_leaf\n",
    "- max_leaf_nodes\n",
    "- min_impurity_decrease\n",
    "- bootstrap\n",
    "- oob_score\n",
    "- n_jobs\n",
    "- random_state\n",
    "- verbose\n",
    "- warm_start\n",
    "- class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators is the amount of tree that you want to create in the forest. the more trees used, the better is the accuracy of the model. But we need to keep in mind that more tree mean more processing power is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterion measures the quality of the split. There are 2 value that can be used for the criterion:\n",
    "- gini, which use the gini impurity to make the split(gini is the default criterion)\n",
    "- entropy, which use the information gain to make the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_features is the maximum number of features looked to find the best split. This parameter increase the result accuracy since each node in the tree looked at a higher range of features. like n_estimators the  processing time will be longer the higher you set the max features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth is used to set how deep you want to create your trees. It is suggested to set the max_depth otherwise your model might run into an overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### min_samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_samples_split set the minimum number of samples that must be available before splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_samples_leaf decide the minimum number of nodes in each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_weight_fraction_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_weight_fraction_leaf is almost the same as min_sample_leaf. min_weight_leaf use a fraction of the sum total number of observation instead of number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_leaf_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_leaf_nodes create the tree in the best-first fashion resulting a relative reduction in the impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_impurity_decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_impurity_decrease is used to split a node if this split induces a decrease of the impurity greater than or equal to this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap is used to boostrap the samples when building a tree. It is suggested to keep bootstrap as default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oob_score is used to decide whether to use out-of-bag samples to estimate the generalization accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_jobs is used to decide the amount of processors that are allowed to be used to process the model. the default value is 1 and it is suggested to put more when it is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_state is a seed for the random number generator. this is used to easily replicate the same result given the same training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose give you constant update when you are building your mode. this is not really usefull since it just takes up stuff in your notebook. It is suggested to leave it as default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### warm_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At False it fits a new forest each time as opposed to when it is True it adds estimators and reuses the solution of the previous fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not put something here it will assume that all classes have a weight of 1, but if you have a multi-output problem a list of dictionaries is used as the columns of y. When the “balanced” mode is used the y values automatically adjust their weights inversely proportional to class frequencies in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing random forest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main way to increase the accuracy of random forest classifier model is by parameter tuning. We can see in the section where we are bulding our random forest classifier we only set the n_estimators and the random state. This cause the model to have a low accuracy since we didnt tune any of the parameters. In this section we will increase the accuracy of our model by using parameter tuning. all the parameter explaination can be seen in the section above or \"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfdeep = RandomForestClassifier(n_estimators = 50, random_state = 10, class_weight='balanced', max_features=10, min_samples_split=5, max_depth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try creating a small forest with a really deep tree and see the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model Accuracy...\n",
      "Training Accuracy =  0.999743658931\n",
      "Test Accuracy =  0.561845655693\n",
      "\n",
      "\n",
      "Printing predicted result...\n",
      "Result_of_Treatment =  [ 1  1 -1 ...,  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "rfdeep.fit(x_train, y_train)\n",
    "y_pred = rfdeep.predict(x_test)  \n",
    "\n",
    "print(\"Getting Model Accuracy...\")\n",
    "\n",
    "# Get regression details\n",
    "#print(\"Estimated Coefficient = \", regressor.coef_)\n",
    "#print(\"Estimated Intercept = \", regressor.intercept_)\n",
    "print(\"Training Accuracy = \", rfdeep.score(x_train, y_train))\n",
    "print(\"Test Accuracy = \", rfdeep.score(x_test, y_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing predicted result...\")\n",
    "print(\"Result_of_Treatment = \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the test result accuracy increased. Now lets try making a huge forest that have a swallow tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Model Accuracy...\n"
     ]
    }
   ],
   "source": [
    "rfswallow = RandomForestClassifier(n_estimators = 1000, random_state = 10, class_weight='balanced', max_features=10, min_samples_split=5, max_depth=10)\n",
    "\n",
    "rfswallow.fit(x_train, y_train)\n",
    "y_pred = rfswallow.predict(x_test)  \n",
    "\n",
    "print(\"Getting Model Accuracy...\")\n",
    "\n",
    "# Get regression details\n",
    "#print(\"Estimated Coefficient = \", regressor.coef_)\n",
    "#print(\"Estimated Intercept = \", regressor.intercept_)\n",
    "print(\"Training Accuracy = \", rfswallow.score(x_train, y_train))\n",
    "print(\"Test Accuracy = \", rfswallow.score(x_test, y_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Printing predicted result...\")\n",
    "print(\"Result_of_Treatment = \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can clearly see the accuracy increase compared to the starting model without parameter tuning. The simplest thing to do in parameter tuning is by keep trying to use diffrent value in the diffrent parameter an find the one suited for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage and disadvatage of random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "- Since decision tree tend to overfit data. when taken the average of all of the tree would help to overcome the overfitting problem. Keep in mind this doesnt mean that overfitting can't happen in random forest.\n",
    "- Random forest is very flexible.\n",
    "- Random forest doesnt require a lot of data pre-processing.\n",
    "- Can mantain accuracy even with a lot of missing data.\n",
    "\n",
    "Disadvantage:\n",
    "- Random forest takes time and effort to create compared to decision tree. this is caused due to the complexity of the random forest.\n",
    "- Random forest require a lot of processing power.\n",
    "- The prediction process of random forest is more time consuming compared to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving underfitting and overfitting problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting refers to a model that can neither model the training data nor generalize to new data. this is caused to the amount of data given as the training set is too little. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. Most of the time model that is underfitted is sacked and a new model is created with diffrent machine learning algorithms.\n",
    "\n",
    "Another solution to this problem is by using oversampling. oversampling is process of increase the data in the dataset by having multiple row od the same data. oversampling match the number of the minority with the manority of the data. for example you have 200 data with \"A\" as the result and 30 as \"B\" result. undersampling will increase the number \"B\" result into 200 by copying data from \"B\" multiple times. therefore instead of using the entire 260 rows of data, oversampling will produce 400 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset that we will be using. we are using a diffrent dataset that you can get from \"https://www.kaggle.com/c/poker-rule-induction/data\". This is done because the result of our original dataset only have 2 results. on the other hand this one have 10 and will show more clearly how undersampling and oversampling work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_class_0, count_class_1, count_class_2, count_class_3, count_class_4, count_class_5, count_class_6, count_class_7, count_class_8, count_class_9 = train_data.hand.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just counting the number of value from the hand collumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0 = train_data[train_data['hand'] == 0]\n",
    "df_class_1 = train_data[train_data['hand'] == 1]\n",
    "df_class_2 = train_data[train_data['hand'] == 2]\n",
    "df_class_3 = train_data[train_data['hand'] == 3]\n",
    "df_class_4 = train_data[train_data['hand'] == 4]\n",
    "df_class_5 = train_data[train_data['hand'] == 5]\n",
    "df_class_6 = train_data[train_data['hand'] == 6]\n",
    "df_class_7 = train_data[train_data['hand'] == 7]\n",
    "df_class_8 = train_data[train_data['hand'] == 8]\n",
    "df_class_9 = train_data[train_data['hand'] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_9_over = df_class_9.sample(count_class_0, replace=True)\n",
    "df_class_8_over = df_class_8.sample(count_class_0, replace=True)\n",
    "df_class_7_over = df_class_7.sample(count_class_0, replace=True)\n",
    "df_class_6_over = df_class_6.sample(count_class_0, replace=True)\n",
    "df_class_5_over = df_class_5.sample(count_class_0, replace=True)\n",
    "df_class_4_over = df_class_4.sample(count_class_0, replace=True)\n",
    "df_class_3_over = df_class_3.sample(count_class_0, replace=True)\n",
    "df_class_2_over = df_class_2.sample(count_class_0, replace=True)\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_2_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_3_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_4_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_5_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_6_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_7_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_8_over], axis=0)\n",
    "df_test_over = pd.concat([df_test_over, df_class_9_over], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random over-sampling:')\n",
    "print(df_test_over.hand.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing number of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_over.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overfitting refers to a model that models the training data too well. Overfittin is caused by the model receive too much data as the training set. This happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "This problem could be solved by using undersampling. Undersampling is process of reducing the data in the dataset. Undersampling match the number of the majority with the minority of the data. for example you have 200 data with \"A\" as the result and 30 as \"B\" result. undersampling will reduce the number \"A\" result into 30 by taking 30 rows of data at random. therefore instead of using the entire 260 rows of data, undersampling will produce 60 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_class_0under = df_class_0.sample(count_class_9, replace=True)\n",
    "df_class_1under = df_class_1.sample(count_class_9, replace=True)\n",
    "df_class_2under = df_class_2.sample(count_class_9, replace=True)\n",
    "df_class_3under = df_class_3.sample(count_class_9, replace=True)\n",
    "df_class_4under = df_class_4.sample(count_class_9, replace=True)\n",
    "df_class_5under = df_class_5.sample(count_class_9, replace=True)\n",
    "df_class_6under = df_class_6.sample(count_class_9, replace=True)\n",
    "df_class_7under = df_class_7.sample(count_class_9, replace=True)\n",
    "df_class_8under = df_class_8.sample(count_class_9, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_under = pd.concat([df_class_9, df_class_0under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_1under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_2under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_3under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_4under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_5under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_6under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_7under], axis=0)\n",
    "df_test_under = pd.concat([df_test_under, df_class_8under], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random under-sampling:')\n",
    "print(df_test_under.hand.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_over.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a good machine learning algorithm. but because the complexity and required to be tuned to suit each model, it is not easy for new user to use them. additionally it require more time for random forest to predict a result. But with all those flaws, random forest is still used widely because the accuracy of the algorithms is really high after a fine tuning of the parameters. \n",
    "\n",
    "This tutorial doesnt dive too deep on how to tune the parameters for random forest. But the creator hope this tutorial will help new user to understand random forest better. Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive.ics.uci.edu. (2019). UCI Machine Learning Repository: Dota2 Games Results Data Set. [online] Available at: https://archive.ics.uci.edu/ml/datasets/Dota2+Games+Results [Accessed 19 Mar. 2019].\n",
    "\n",
    "Brownlee, J. (2019). Overfitting and Underfitting With Machine Learning Algorithms. [online] Machine Learning Mastery. Available at: https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/ [Accessed 21 Mar. 2019].\n",
    "\n",
    "Kaggle.com. (2019). Poker Rule Induction | Kaggle. [online] Available at: https://www.kaggle.com/c/poker-rule-induction/data [Accessed 21 Mar. 2019].\n",
    "\n",
    "Marco Altini. (2019). Dealing with imbalanced data: undersampling, oversampling and proper cross-validation. [online] Available at: https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation [Accessed 21 Mar. 2019].\n",
    "\n",
    "Measuringu.com. (2019). MeasuringU: 7 Ways to Handle Missing Data. [online] Available at: https://measuringu.com/handle-missing-data/ [Accessed 20 Mar. 2019].\n",
    "\n",
    "Medium. (2019). Tuning a Random Forest Classifier. [online] Available at: https://medium.com/@taplapinger/tuning-a-random-forest-classifier-1b252d1dde92 [Accessed 21 Mar. 2019].\n",
    "\n",
    "Sas.com. (2019). A guide to machine learning algorithms and their applications. [online] Available at: https://www.sas.com/en_ie/insights/articles/analytics/machine-learning-algorithms.html [Accessed 19 Mar. 2019].\n",
    "\n",
    "Sas.com. (2019). Machine Learning: What it is and why it matters. [online] Available at: https://www.sas.com/en_ie/insights/analytics/machine-learning.html [Accessed 19 Mar. 2019].\n",
    "\n",
    "Scikit-learn.org. (2019). 3.2.4.3.1. sklearn.ensemble.RandomForestClassifier — scikit-learn 0.20.3 documentation. [online] Available at: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html [Accessed 21 Mar. 2019].\n",
    "\n",
    "Techopedia.com. (2019). What is Data Preprocessing? - Definition from Techopedia. [online] Available at: https://www.techopedia.com/definition/14650/data-preprocessing [Accessed 20 Mar. 2019].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
